{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Assignment No.: 8 Reinforcement Learning \n",
    "\n",
    "Problem Statement: Implement reinforcement learning and make a simple demonstrative game to show reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory\n",
    "\n",
    "Reinforcement Learning is a feedback-based Machine learning technique in which an agent learns to behave in an environment by performing the actions and seeing the results of actions. For each good action, the agent gets positive feedback, and for each bad action, the agent gets negative feedback or penalty.\n",
    "In Reinforcement Learning, the agent learns automatically using feedbacks without any labeled data, unlike supervised learning.\n",
    "Since there is no labeled data, so the agent is bound to learn by its experience only.\n",
    "RL solves a specific type of problem where decision making is sequential, and the goal is long-term, such as game-playing, robotics, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation \n",
    "\n",
    "This code demonstrates a simple example of reinforcement learning in a grid world environment. Reinforcement learning is a type of machine learning where an agent learns to make a sequence of decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions. Over time, the agent learns an optimal policy that maximizes its cumulative reward.\n",
    "\n",
    "Grid World Environment:\n",
    "\n",
    "The environment is represented as a grid world, which is a common scenario in reinforcement learning.\n",
    "The grid world is a 2D space where the agent can move around to reach a goal while avoiding obstacles.\n",
    "In this example, the grid is represented as a linear grid for simplicity, where the agent moves from one state to another.\n",
    "\n",
    "\n",
    "Training the Agent:\n",
    "\n",
    "The agent starts at a specific initial state and learns to navigate to the goal state.\n",
    "It uses a Q-learning algorithm to update its Q-values based on the rewards and penalties it receives during its exploration.\n",
    "The Q-values represent the agent's estimation of the expected cumulative rewards for each state-action pair.\n",
    "\n",
    "\n",
    "Exploration and Exploitation:\n",
    "\n",
    "The agent has a trade-off between exploration and exploitation. It explores the environment by taking random actions with a certain probability and exploits its learned Q-values to make optimal decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's path: [0, 5, 10, 15, 16, 17, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid world and RL parameters\n",
    "\n",
    "GRID_SIZE = 5\n",
    "START = 0\n",
    "GOAL = 24  # Assuming a linear grid world\n",
    "OBSTACLES = [6, 12, 13, 18]\n",
    "NUM_STATES = GRID_SIZE * GRID_SIZE\n",
    "NUM_ACTIONS = 4  # Up, Down, Left, Right\n",
    "NUM_EPISODES = 1000\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "EXPLORATION_PROB = 0.2\n",
    "\n",
    "# Create a Q-table\n",
    "\n",
    "q_table = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
    "\n",
    "# Function to take an action\n",
    "\n",
    "def take_action(state, action):\n",
    "    row, col = divmod(state, GRID_SIZE)\n",
    "\n",
    "    if action == 0 and row > 0:  # Up\n",
    "        return state - GRID_SIZE\n",
    "    elif action == 1 and row < GRID_SIZE - 1:  # Down\n",
    "        return state + GRID_SIZE\n",
    "    elif action == 2 and col > 0:  # Left\n",
    "        return state - 1\n",
    "    elif action == 3 and col < GRID_SIZE - 1:  # Right\n",
    "        return state + 1\n",
    "    else:\n",
    "        return state  # Stay in the same state if the action would go out of bounds\n",
    "\n",
    "\n",
    "# Q-learning algorithm\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state = START\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.uniform(0, 1) < EXPLORATION_PROB:\n",
    "            action = np.random.choice([0, 1, 2, 3])  # Choose a random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        next_state = take_action(state, action)\n",
    "\n",
    "        if next_state in OBSTACLES:\n",
    "            reward = -1\n",
    "        elif next_state == GOAL:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        q_table[state][action] = (1 - LEARNING_RATE) * q_table[state][action] + \\\n",
    "            LEARNING_RATE * (reward + DISCOUNT_FACTOR * np.max(q_table[next_state]))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if state == GOAL:\n",
    "            done = True\n",
    "\n",
    "# Test the trained agent\n",
    "\n",
    "state = START\n",
    "path = [state]\n",
    "\n",
    "while state != GOAL:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state = take_action(state, action)\n",
    "    path.append(state)\n",
    "    \n",
    "\n",
    "print(\"Agent's path:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "Reinforcement learning is a powerful paradigm that allows agents to learn from their interactions with the environment. In this code, the agent learns to navigate the grid world by trial and error, gradually discovering the most rewarding path while avoiding obstacles. The Q-learning algorithm is a fundamental technique within reinforcement learning that helps the agent make informed decisions. This example serves as a basic introduction to the principles of reinforcement learning in a grid world context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
