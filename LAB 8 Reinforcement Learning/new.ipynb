{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world and RL parameters\n",
    "\n",
    "GRID_SIZE = 5\n",
    "START = (0, 0)\n",
    "GOAL = (4, 4)\n",
    "OBSTACLES = [(1, 1), (2, 2), (3, 2)]\n",
    "NUM_EPISODES = 1000\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "EXPLORATION_PROB = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Q-table\n",
    "\n",
    "q_table = np.zeros((GRID_SIZE, GRID_SIZE, 4))  # Up, Down, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take an action in the environment\n",
    "def take_action(state, action):\n",
    "    x, y = state\n",
    "\n",
    "    if action == 'up':\n",
    "        x = max(0, x - 1)\n",
    "    elif action == 'down':\n",
    "        x = min(GRID_SIZE - 1, x + 1)\n",
    "    elif action == 'left':\n",
    "        y = max(0, y - 1)\n",
    "    elif action == 'right':\n",
    "        y = min(GRID_SIZE - 1, y + 1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\SEM 7\\Machine Learning LAB\\LAB 8 Reinforcement Learning\\new.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m q_table[state][action] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m LEARNING_RATE) \u001b[39m*\u001b[39m q_table[state][action] \u001b[39m+\u001b[39m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     LEARNING_RATE \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m DISCOUNT_FACTOR \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(q_table[next_state]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m GOAL:\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Q-learning algorithm\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state = START\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.uniform(0, 1) < EXPLORATION_PROB:\n",
    "            action = np.random.choice(['up', 'down', 'left', 'right'])\n",
    "        else:\n",
    "            action = ['up', 'down', 'left', 'right'][np.argmax(q_table[state])]\n",
    "\n",
    "        next_state = take_action(state, action)\n",
    "\n",
    "        if next_state in OBSTACLES:\n",
    "            reward = -1\n",
    "        elif next_state == GOAL:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "\n",
    "        q_table[state][action] = (1 - LEARNING_RATE) * q_table[state][action] + \\\n",
    "            LEARNING_RATE * (reward + DISCOUNT_FACTOR * np.max(q_table[next_state]))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if state == GOAL:\n",
    "            done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\SEM 7\\Machine Learning LAB\\LAB 8 Reinforcement Learning\\new.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m path \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwhile\u001b[39;00m state \u001b[39m!=\u001b[39m GOAL:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     action \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mup\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdown\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m][np\u001b[39m.\u001b[39;49margmax(q_table[state])]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     state \u001b[39m=\u001b[39m take_action(state, action)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SEM%207/Machine%20Learning%20LAB/LAB%208%20Reinforcement%20Learning/new.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     path\u001b[39m.\u001b[39mappend(state)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmax\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "\n",
    "state = START\n",
    "path = []\n",
    "\n",
    "while state != GOAL:\n",
    "    action = ['up', 'down', 'left', 'right'][np.argmax(q_table[state])]\n",
    "    state = take_action(state, action)\n",
    "    path.append(state)\n",
    "\n",
    "print(\"Agent's path:\", path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
