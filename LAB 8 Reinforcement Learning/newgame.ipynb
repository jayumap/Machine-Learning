{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the grid or the gameboard\n",
    "# Here S: start state, W: walls, G: goal state\n",
    "\n",
    "grid = np.array([['S', '.', '.', 'W'],\n",
    "                 ['.', 'W', '.', 'W'],\n",
    "                 ['.', '.', '.', '.'],\n",
    "                 ['W', '.', 'W', 'G']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining actions \n",
    "\n",
    "actions = ['U', 'D', 'L', 'R']\n",
    "numActions = len(actions)\n",
    "numActions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Q table \n",
    "\n",
    "The Q-table is a fundamental data structure used in the Q-learning algorithm. It stores the learned values associated with state-action pairs. The Q-value (short for \"quality value\") in the Q-table represents the expected cumulative reward an agent can achieve by taking a particular action in a specific state and then following an optimal policy thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize q table with zeroes initially\n",
    "\n",
    "numStates = grid.size\n",
    "qTable = np.zeros((numStates, numActions))\n",
    "qTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': 0, '.': 13, 'W': 14, 'G': 15}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a mapping between states and their indices \n",
    "\n",
    "stateIndices = {\n",
    "    state: index for index, \n",
    "    state in enumerate(grid.flatten())\n",
    "}\n",
    "stateIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters\n",
    "\n",
    "learningRate = 0.1\n",
    "discountFactor = 0.9\n",
    "explorationProb = 0.3\n",
    "numEpisodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take action and return the new state\n",
    "def takeAction(state, action):\n",
    "    currentRow, currentCol = np.where(grid == state)\n",
    "    currentRow, currentCol = currentRow[0], currentCol[0]\n",
    "\n",
    "    if action == 'U':\n",
    "        newRow, newCol = currentRow - 1, currentCol\n",
    "    elif action == 'D':\n",
    "        newRow, newCol = currentRow + 1, currentCol\n",
    "    elif action == 'L':\n",
    "        newRow, newCol = currentRow, currentCol - 1\n",
    "    elif action == 'R':\n",
    "        newRow, newCol = currentRow, currentCol + 1\n",
    "\n",
    "    if 0 <= newRow < grid.shape[0] and 0 <= newCol < grid.shape[1]:\n",
    "        return grid[newRow, newCol]\n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Q-learning loop\n",
    "for episode in range(numEpisodes):\n",
    "    currentState = 'S'\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if random.random() < explorationProb:\n",
    "            action = random.choice(actions)  # Exploration\n",
    "        else:\n",
    "            action = actions[np.argmax(qTable[stateIndices[currentState]])]  # Exploitation\n",
    "\n",
    "        newState = takeAction(currentState, action)\n",
    "        reward = -1 if newState != 'W' else -5  # Define rewards\n",
    "\n",
    "        qTable[stateIndices[currentState], actions.index(action)] += \\\n",
    "            learningRate * (reward + discountFactor * (np.max(qTable[stateIndices[newState]]) -\n",
    "            qTable[stateIndices[currentState], actions.index(action)]))\n",
    "\n",
    "        if newState == 'G':\n",
    "            done = True\n",
    "        else:\n",
    "            current_state = newState"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
